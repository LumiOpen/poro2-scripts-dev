Submodule Megatron-LM contains modified content
diff --git a/Megatron-LM/megatron/core/dist_checkpointing/strategies/filesystem_async.py b/Megatron-LM/megatron/core/dist_checkpointing/strategies/filesystem_async.py
index 47ab4d11..cd93c33a 100644
--- a/Megatron-LM/megatron/core/dist_checkpointing/strategies/filesystem_async.py
+++ b/Megatron-LM/megatron/core/dist_checkpointing/strategies/filesystem_async.py
@@ -22,6 +22,15 @@ from torch.futures import Future
 
 logger = logging.getLogger(__name__)
 
+try:
+    # This PR https://github.com/pytorch/pytorch/pull/143359 introduced breaking change to saving checkpoints
+    # in torch_dist format. This is a workaround to fix the issue.
+    from torch.distributed.checkpoint.filesystem import _StorageWriterTransforms
+    from functools import partial
+    _write_item = partial(_write_item, _StorageWriterTransforms())
+except ImportError:
+    pass
+
 WriteBucket = Tuple[Path, str, Tuple[list, list]]  # represents writes to a single file
 
 _results_queue = None
diff --git a/Megatron-LM/megatron/training/arguments.py b/Megatron-LM/megatron/training/arguments.py
index d8d08f61..aeb29aac 100644
--- a/Megatron-LM/megatron/training/arguments.py
+++ b/Megatron-LM/megatron/training/arguments.py
@@ -706,7 +706,7 @@ def validate_args(args, defaults={}):
     if args.num_experts is not None:
         assert args.spec is None, "Model Spec must be None when using MoEs"
     
-    if args.tensor_model_parallel_size > 1:
+    if args.tensor_model_parallel_size > 1 and args.num_experts:
             assert args.sequence_parallel, \
                 "When using MoE and tensor parallelism, sequence parallelism must be used."
                 
diff --git a/Megatron-LM/tools/checkpoint/loader_llama_mistral.py b/Megatron-LM/tools/checkpoint/loader_llama_mistral.py
index b6697964..980ab77f 100644
--- a/Megatron-LM/tools/checkpoint/loader_llama_mistral.py
+++ b/Megatron-LM/tools/checkpoint/loader_llama_mistral.py
@@ -459,6 +459,8 @@ def _load_checkpoint(queue, args):
                 '--no-initialization',
                 '--load', args.load_dir,
                 '--no-one-logger',
+                '--no-gradient-accumulation-fusion',
+                '--ckpt-format', 'torch',
                 ]
 
     if args.make_vocab_size_divisible_by is not None:
diff --git a/Megatron-LM/tools/checkpoint/loader_mcore.py b/Megatron-LM/tools/checkpoint/loader_mcore.py
index 9185969b..0b07ddfe 100644
--- a/Megatron-LM/tools/checkpoint/loader_mcore.py
+++ b/Megatron-LM/tools/checkpoint/loader_mcore.py
@@ -93,6 +93,8 @@ def _load_checkpoint(queue, args):
     margs.use_legacy_models = False
     margs.transformer_impl = args.loader_transformer_impl
 
+    margs.tensor_model_parallel_size = checkpoint_args.tensor_model_parallel_size
+    margs.pipeline_model_parallel_size = checkpoint_args.pipeline_model_parallel_size
     def check_for_arg(arg_name, default=None):
         if getattr(margs, arg_name, None) is None:
             if default is not None:
